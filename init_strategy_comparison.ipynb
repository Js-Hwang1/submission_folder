{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CircuitKV Initialization Strategy Comparison\n",
    "**Goal:** Find optimal prefill initialization to close gap with H2O on summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers accelerate \"datasets<3.0\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "TOP_K_PERCENT = 0.20\n",
    "N_SAMPLES = 3\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (full precision fp16)\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    output_attentions=True,\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded (fp16).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load real LongBench NarrativeQA\nprint(\"Loading LongBench NarrativeQA...\")\n\ndataset = load_dataset(\n    \"THUDM/LongBench\",\n    \"narrativeqa\",\n    split=\"test\",\n    trust_remote_code=True,\n)\n\nsamples = [dataset[i] for i in range(N_SAMPLES)]\nprint(f\"Loaded {len(samples)} samples\")\nprint(f\"Sample keys: {samples[0].keys()}\")\nprint(f\"Context length (chars): {len(samples[0]['context'])}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = x - x.min()\n",
    "    return x / (x.max() + 1e-10)\n",
    "\n",
    "def jaccard_topk(scores, oracle, k_percent=0.20):\n",
    "    k = int(len(scores) * k_percent)\n",
    "    top_scores = set(torch.argsort(scores, descending=True)[:k].tolist())\n",
    "    top_oracle = set(torch.argsort(oracle, descending=True)[:k].tolist())\n",
    "    return len(top_scores & top_oracle) / len(top_scores | top_oracle)\n",
    "\n",
    "def compute_strategies(h2o, query_attn):\n",
    "    h2o_norm = normalize(h2o)\n",
    "    query_norm = normalize(query_attn)\n",
    "    \n",
    "    return {\n",
    "        \"H2O (Blind)\": h2o_norm,\n",
    "        \"Additive\": 0.5 * h2o_norm + 0.5 * query_norm,\n",
    "        \"Max\": torch.maximum(h2o_norm, query_norm),\n",
    "        \"Multiplicative\": torch.sqrt(h2o_norm * query_norm + 1e-10),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_sample(model, tokenizer, sample):\n",
    "    context = sample[\"context\"][:12000]\n",
    "    question = sample[\"input\"]\n",
    "    \n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "    input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "    seq_len = input_ids.shape[1]\n",
    "    \n",
    "    outputs = model(input_ids, output_attentions=True)\n",
    "    attn = outputs.attentions[-1][0].mean(dim=0).float().cpu()\n",
    "    \n",
    "    h2o = attn.sum(dim=0)\n",
    "    query_idx = seq_len - 1\n",
    "    query_attn = attn[query_idx, :]\n",
    "    oracle = query_attn.clone()\n",
    "    \n",
    "    strategies = compute_strategies(h2o, query_attn)\n",
    "    \n",
    "    results = {}\n",
    "    for name, scores in strategies.items():\n",
    "        results[name] = jaccard_topk(scores, oracle, TOP_K_PERCENT)\n",
    "    \n",
    "    return results, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "all_results = {name: [] for name in [\"H2O (Blind)\", \"Additive\", \"Max\", \"Multiplicative\"]}\n",
    "\n",
    "print(\"Evaluating samples...\\n\")\n",
    "for i, sample in enumerate(tqdm(samples)):\n",
    "    results, seq_len = evaluate_sample(model, tokenizer, sample)\n",
    "    for name, score in results.items():\n",
    "        all_results[name].append(score)\n",
    "    print(f\"Sample {i+1} (len={seq_len}): {results}\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"        INITIALIZATION STRATEGY COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Strategy':<20} {'Jaccard@20%':>15}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "final_scores = {}\n",
    "for name in [\"H2O (Blind)\", \"Additive\", \"Max\", \"Multiplicative\"]:\n",
    "    mean_score = np.mean(all_results[name])\n",
    "    final_scores[name] = mean_score\n",
    "    marker = \" â˜…\" if mean_score == max(np.mean(all_results[n]) for n in all_results) else \"\"\n",
    "    print(f\"{name:<20} {mean_score:>15.4f}{marker}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "best = max(final_scores, key=final_scores.get)\n",
    "print(f\"\\nWinner: {best}\")\n",
    "\n",
    "if final_scores[\"Multiplicative\"] > final_scores[\"H2O (Blind)\"]:\n",
    "    gain = (final_scores[\"Multiplicative\"] - final_scores[\"H2O (Blind)\"]) / final_scores[\"H2O (Blind)\"] * 100\n",
    "    print(f\"Multiplicative vs H2O: +{gain:.1f}% relative improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    PER-SAMPLE BREAKDOWN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Sample':<10} {'H2O':<12} {'Additive':<12} {'Max':<12} {'Multiplicative':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    h = all_results[\"H2O (Blind)\"][i]\n",
    "    a = all_results[\"Additive\"][i]\n",
    "    m = all_results[\"Max\"][i]\n",
    "    g = all_results[\"Multiplicative\"][i]\n",
    "    print(f\"{i+1:<10} {h:<12.4f} {a:<12.4f} {m:<12.4f} {g:<12.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "strategies = list(final_scores.keys())\n",
    "scores = list(final_scores.values())\n",
    "colors = ['#3498db', '#9b59b6', '#e67e22', '#2ecc71']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(strategies, scores, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "max_idx = scores.index(max(scores))\n",
    "bars[max_idx].set_edgecolor('red')\n",
    "bars[max_idx].set_linewidth(3)\n",
    "\n",
    "plt.ylabel('Jaccard@20% Overlap with Oracle', fontsize=12)\n",
    "plt.title('Initialization Strategy Comparison (LongBench NarrativeQA)', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, max(scores) * 1.2)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{score:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}