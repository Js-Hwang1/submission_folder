{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"datasets<=3.0\" rouge_score transformers accelerate\n",
    "!huggingface-cli login --token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F, math, json, re, string\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv\n",
    "from tqdm import tqdm\n",
    "import types\n",
    "\n",
    "MODEL = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "BUDGET, WINDOW, SINK = 2048, 64, 4\n",
    "SAMPLES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "# Force eager attention (disable FlashAttention) to ensure consistent behavior\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto',\n",
    "    attn_implementation='eager'  # Force standard attention\n",
    ")\n",
    "base_model.eval()\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Verify the attention class\n",
    "print(f\"Attention class: {type(base_model.model.layers[0].self_attn)}\")\n",
    "print(f\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_h2o(attn, window, sink):\n",
    "    # attn: [bsz, heads, seq_len, seq_len] - FULL attention matrix with ALL queries\n",
    "    # Sum across ALL query positions (dim=-2), average across batch and heads\n",
    "    # This matches KVCache-Factory H2O exactly (line 554 in pyramidkv_utils.py)\n",
    "    return attn[:, :, :, :-window].sum(dim=-2).mean(dim=(0,1))\n",
    "\n",
    "def atom_spectral(attn, window, sink):\n",
    "    # attn: [bsz, heads, seq_len, seq_len] - FULL attention matrix\n",
    "    # Compute dominant eigenvector of attention matrix\n",
    "    A = attn.mean(dim=(0,1))[:, :-window]\n",
    "    v = torch.ones(A.shape[1], device=A.device, dtype=A.dtype)\n",
    "    for _ in range(10):\n",
    "        v = A.T @ (A @ v)\n",
    "        v = v / (v.norm() + 1e-8)\n",
    "    return v\n",
    "\n",
    "def atom_sharpness(attn, window, sink):\n",
    "    # attn: [bsz, heads, seq_len, seq_len] - FULL attention matrix\n",
    "    # Measure entropy - low entropy = sharp/focused attention\n",
    "    a = attn.mean(dim=(0,1))[:, :-window]\n",
    "    a = a / (a.sum(dim=0, keepdim=True) + 1e-8)\n",
    "    ent = -(a * (a + 1e-8).log()).sum(dim=0)\n",
    "    return math.log(window) - ent\n",
    "\n",
    "def atom_combined(attn, window, sink):\n",
    "    h = atom_h2o(attn, window, sink)\n",
    "    s = atom_sharpness(attn, window, sink)\n",
    "    h = h / (h.max() + 1e-8)\n",
    "    s = s / (s.max() + 1e-8)\n",
    "    return torch.maximum(h, s)\n",
    "\n",
    "def bond_walker(attn, window, sink, walkers=512, steps=100):\n",
    "    # attn: [bsz, heads, seq_len, seq_len] - FULL attention matrix\n",
    "    # Random walk from last position backward following attention edges\n",
    "    A = attn.mean(dim=(0,1))  # [seq_len, seq_len]\n",
    "    seq_len = A.shape[0]\n",
    "    counts = torch.zeros(seq_len, device=A.device)\n",
    "    \n",
    "    for _ in range(walkers):\n",
    "        pos = seq_len - 1  # Start at last query position\n",
    "        for _ in range(steps):\n",
    "            counts[pos] += 1\n",
    "            if pos < sink: break  # Absorbed at sink\n",
    "            \n",
    "            # Sample next position based on attention from current query\n",
    "            probs = A[pos, :pos+1]  # Attention to keys 0..pos (causal)\n",
    "            if probs.sum() < 1e-8: break\n",
    "            pos = torch.multinomial(probs / probs.sum(), 1).item()\n",
    "    \n",
    "    return counts[:-window]\n",
    "\n",
    "def bond_multisource(attn, window, sink, walkers=256, steps=100):\n",
    "    # attn: [bsz, heads, seq_len, seq_len] - FULL attention matrix\n",
    "    # Random walks starting from ALL positions in the recent window\n",
    "    A = attn.mean(dim=(0,1))  # [seq_len, seq_len]\n",
    "    seq_len = A.shape[0]\n",
    "    counts = torch.zeros(seq_len, device=A.device)\n",
    "    \n",
    "    for window_pos in range(window):\n",
    "        start_pos = seq_len - window + window_pos  # Position in recent window\n",
    "        for _ in range(walkers // window):\n",
    "            pos = start_pos\n",
    "            for _ in range(steps):\n",
    "                counts[pos] += 1\n",
    "                if pos < sink: break  # Absorbed at sink\n",
    "                \n",
    "                # Sample next position based on attention from current query\n",
    "                probs = A[pos, :pos+1]  # Attention to keys 0..pos (causal)\n",
    "                if probs.sum() < 1e-8: break\n",
    "                pos = torch.multinomial(probs / probs.sum(), 1).item()\n",
    "    \n",
    "    return counts[:-window]\n",
    "\n",
    "ATOMS = {'h2o': atom_h2o, 'spectral': atom_spectral, 'sharpness': atom_sharpness, 'combined': atom_combined}\n",
    "BONDS = {'none': lambda a,w,s: torch.zeros_like(atom_h2o(a,w,s)), 'walker': bond_walker, 'multi': bond_multisource}\n",
    "\n",
    "def combine_static(atom, bond, budget, ratio=0.8):\n",
    "    atom = atom / (atom.max() + 1e-8)\n",
    "    bond = bond / (bond.max() + 1e-8)\n",
    "    n_atom = int(budget * ratio)\n",
    "    n_bond = budget - n_atom\n",
    "    atom_idx = atom.topk(min(n_atom, len(atom))).indices\n",
    "    mask = torch.ones_like(bond, dtype=torch.bool)\n",
    "    mask[atom_idx] = False\n",
    "    bond_idx = bond[mask].topk(min(n_bond, mask.sum())).indices\n",
    "    keep = torch.zeros(len(atom), dtype=torch.bool, device=atom.device)\n",
    "    keep[atom_idx] = True\n",
    "    keep[mask.nonzero(as_tuple=True)[0][bond_idx]] = True\n",
    "    return keep\n",
    "\n",
    "def combine_max(atom, bond, budget):\n",
    "    atom = atom / (atom.max() + 1e-8)\n",
    "    bond = bond / (bond.max() + 1e-8)\n",
    "    scores = torch.maximum(atom, bond)\n",
    "    idx = scores.topk(min(budget, len(scores))).indices\n",
    "    keep = torch.zeros(len(atom), dtype=torch.bool, device=atom.device)\n",
    "    keep[idx] = True\n",
    "    return keep\n",
    "\n",
    "def combine_dynamic(atom, bond, budget):\n",
    "    atom_top = set(atom.topk(min(budget, len(atom))).indices.tolist())\n",
    "    bond_top = set(bond.topk(min(budget, len(bond))).indices.tolist())\n",
    "    overlap = len(atom_top & bond_top) / budget if budget > 0 else 0\n",
    "    ratio = 0.6 + 0.3 * overlap\n",
    "    return combine_static(atom, bond, budget, ratio)\n",
    "\n",
    "COMBINES = {'static80': lambda a,b,n: combine_static(a,b,n,0.8), 'static70': lambda a,b,n: combine_static(a,b,n,0.7), \n",
    "            'max': combine_max, 'dynamic': combine_dynamic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original forward methods\n",
    "_original_forwards = {}\n",
    "_debug_eviction = {'count': 0}\n",
    "\n",
    "def patch_model(model, atom_fn, bond_fn, combine_fn, budget=2048, window=64, sink=4):\n",
    "    \"\"\"Patch model attention layers for KV eviction. Follows KVCache-Factory structure.\"\"\"\n",
    "    _debug_eviction['count'] = 0\n",
    "\n",
    "    config = model.config\n",
    "    \n",
    "    # Get the rotary embedding from the model (location varies by transformers version)\n",
    "    if hasattr(model.model.layers[0].self_attn, 'rotary_emb'):\n",
    "        rotary_emb = model.model.layers[0].self_attn.rotary_emb\n",
    "    else:\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "\n",
    "    # Compute attention dimensions from config\n",
    "    num_heads = config.num_attention_heads\n",
    "    num_key_value_heads = getattr(config, 'num_key_value_heads', num_heads)\n",
    "    head_dim = config.hidden_size // num_heads\n",
    "    num_key_value_groups = num_heads // num_key_value_heads\n",
    "    hidden_size = config.hidden_size\n",
    "\n",
    "    # Store config on each attention module\n",
    "    for layer_idx in range(len(model.model.layers)):\n",
    "        attn = model.model.layers[layer_idx].self_attn\n",
    "        attn._poc_atom_fn = atom_fn\n",
    "        attn._poc_bond_fn = bond_fn\n",
    "        attn._poc_combine_fn = combine_fn\n",
    "        attn._poc_budget = budget\n",
    "        attn._poc_window = window\n",
    "        attn._poc_sink = sink\n",
    "        attn._poc_kv_seq_len = 0\n",
    "        attn._poc_rotary_emb = rotary_emb\n",
    "        # Store computed values\n",
    "        attn._poc_num_heads = num_heads\n",
    "        attn._poc_num_key_value_heads = num_key_value_heads\n",
    "        attn._poc_head_dim = head_dim\n",
    "        attn._poc_num_key_value_groups = num_key_value_groups\n",
    "        attn._poc_hidden_size = hidden_size\n",
    "\n",
    "    def patched_forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask = None,\n",
    "        position_ids = None,\n",
    "        past_key_value = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position = None,\n",
    "        position_embeddings = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        \n",
    "        # Get all config from stored attributes\n",
    "        atom_fn = self._poc_atom_fn\n",
    "        bond_fn = self._poc_bond_fn\n",
    "        combine_fn = self._poc_combine_fn\n",
    "        budget = self._poc_budget\n",
    "        window = self._poc_window\n",
    "        sink = self._poc_sink\n",
    "        rotary_emb = self._poc_rotary_emb\n",
    "        num_heads = self._poc_num_heads\n",
    "        num_key_value_heads = self._poc_num_key_value_heads\n",
    "        head_dim = self._poc_head_dim\n",
    "        num_key_value_groups = self._poc_num_key_value_groups\n",
    "        hidden_size = self._poc_hidden_size\n",
    "\n",
    "        # Project Q, K, V\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # Track KV sequence length\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            if self._poc_kv_seq_len != 0:\n",
    "                kv_seq_len += self._poc_kv_seq_len\n",
    "            else:\n",
    "                kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "\n",
    "        # Apply RoPE\n",
    "        if position_embeddings is None:\n",
    "            cos, sin = rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        \n",
    "        # Expand KV for GQA\n",
    "        key_states = repeat_kv(key_states, num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, num_key_value_groups)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            is_prefill = (key_states.shape[-2] == kv_seq_len)\n",
    "            \n",
    "            if is_prefill and q_len >= budget:\n",
    "                # PREFILL WITH EVICTION\n",
    "                self._poc_kv_seq_len = kv_seq_len\n",
    "                \n",
    "                if self.layer_idx == 0 and _debug_eviction['count'] == 0:\n",
    "                    print(f'[EVICTION] q_len={q_len} >= budget={budget}')\n",
    "                    _debug_eviction['count'] += 1\n",
    "                \n",
    "                # Compute attention for scoring\n",
    "                attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "                \n",
    "                # Causal mask for window\n",
    "                mask = torch.full((window, window), torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n",
    "                mask_cond = torch.arange(window, device=attn_weights.device)\n",
    "                mask.masked_fill_(mask_cond < (mask_cond + 1).view(window, 1), 0)\n",
    "                attn_weights[:, :, -window:, -window:] += mask[None, None, :, :]\n",
    "                \n",
    "                attn_weights_for_score = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "                \n",
    "                # Score tokens\n",
    "                atom = atom_fn(attn_weights_for_score, window, sink)\n",
    "                bond = bond_fn(attn_weights_for_score, window, sink)\n",
    "                \n",
    "                # Keep: sink + selected middle + window\n",
    "                atom_middle = atom[sink:]\n",
    "                bond_middle = bond[sink:]\n",
    "                keep_n = budget - window - sink\n",
    "                keep_middle = combine_fn(atom_middle, bond_middle, min(keep_n, len(atom_middle)))\n",
    "                \n",
    "                keep_mask = torch.zeros(q_len, dtype=torch.bool, device=key_states.device)\n",
    "                keep_mask[:sink] = True\n",
    "                keep_mask[sink:sink+len(keep_middle)] = keep_middle\n",
    "                keep_mask[-window:] = True\n",
    "                \n",
    "                keep_indices = keep_mask.nonzero(as_tuple=True)[0]\n",
    "                idx = keep_indices.view(1, 1, -1, 1).expand(bsz, num_heads, -1, head_dim)\n",
    "                \n",
    "                key_compress = key_states.gather(dim=2, index=idx)\n",
    "                val_compress = value_states.gather(dim=2, index=idx)\n",
    "                past_key_value.update(key_compress, val_compress, self.layer_idx, cache_kwargs)\n",
    "                \n",
    "            elif is_prefill:\n",
    "                # PREFILL WITHOUT EVICTION\n",
    "                self._poc_kv_seq_len = kv_seq_len\n",
    "                past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "            else:\n",
    "                # GENERATION\n",
    "                self._poc_kv_seq_len += q_len\n",
    "                key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "            \n",
    "            past_key_value._seen_tokens = self._poc_kv_seq_len\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights + attention_mask[:, :, :, :key_states.shape[-2]]\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().reshape(bsz, q_len, hidden_size)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, None if not output_attentions else attn_weights, past_key_value\n",
    "\n",
    "    # Patch all layers\n",
    "    for layer_idx in range(len(model.model.layers)):\n",
    "        model.model.layers[layer_idx].self_attn.forward = types.MethodType(\n",
    "            patched_forward, model.model.layers[layer_idx].self_attn\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "def reset_model(model):\n",
    "    \"\"\"Reset kv_seq_len for each layer before new generation.\"\"\"\n",
    "    for layer in model.model.layers:\n",
    "        if hasattr(layer.self_attn, '_poc_kv_seq_len'):\n",
    "            layer.self_attn._poc_kv_seq_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_with_eviction(prompt, model, max_new=128):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Reset KV sequence tracking before each generation\n",
    "    reset_model(model)\n",
    "    \n",
    "    inp = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=7500).to('cuda')\n",
    "    seq_len = inp.input_ids.shape[1]\n",
    "    \n",
    "    out = model.generate(\n",
    "        **inp, \n",
    "        max_new_tokens=max_new, \n",
    "        do_sample=False, \n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        use_cache=True\n",
    "    )\n",
    "    result = tokenizer.decode(out[0][seq_len:], skip_special_tokens=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    return result\n",
    "\n",
    "def normalize(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score_tokens(prediction_tokens, ground_truth_tokens):\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def qa_f1(pred, gold):\n",
    "    normalized_pred = normalize(pred)\n",
    "    normalized_gold = normalize(gold)\n",
    "    pred_tokens = normalized_pred.split()\n",
    "    gold_tokens = normalized_gold.split()\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0\n",
    "    return f1_score_tokens(pred_tokens, gold_tokens)\n",
    "\n",
    "def eval_task(task, atom_fn, bond_fn, combine_fn, n=SAMPLES):\n",
    "    model = patch_model(base_model, atom_fn, bond_fn, combine_fn, BUDGET, WINDOW, SINK)\n",
    "    \n",
    "    task2maxlen = {'qasper': 128, 'narrativeqa': 128, 'hotpotqa': 32}\n",
    "    max_new = task2maxlen.get(task, 64)\n",
    "    \n",
    "    ds = load_dataset('THUDM/LongBench', task, split='test', trust_remote_code=True)\n",
    "    scores = []\n",
    "    for i, item in enumerate(tqdm(ds, total=min(n, len(ds)))):\n",
    "        if i >= n: break\n",
    "        if task == 'qasper':\n",
    "            prompt = f\"You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \\\"unanswerable\\\". If the question is a yes/no question, answer \\\"yes\\\", \\\"no\\\", or \\\"unanswerable\\\". Do not provide any explanation.\\n\\nArticle: {item['context']}\\n\\n Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \\\"unanswerable\\\". If the question is a yes/no question, answer \\\"yes\\\", \\\"no\\\", or \\\"unanswerable\\\". Do not provide any explanation.\\n\\nQuestion: {item['input']}\\n\\nAnswer:\"\n",
    "        elif task == 'narrativeqa':\n",
    "            prompt = f\"You are given a story, which can be either a novel or a movie script, and a question. Answer the question asconcisely as you can, using a single phrase if possible. Do not provide any explanation.\\n\\nStory: {item['context']}\\n\\nNow, answer the question based on the story asconcisely as you can, using a single phrase if possible. Do not provide any explanation.\\n\\nQuestion: {item['input']}\\n\\nAnswer:\"\n",
    "        else:\n",
    "            prompt = f\"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{item['context']}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {item['input']}\\nAnswer:\"\n",
    "        \n",
    "        pred = generate_with_eviction(prompt, model, max_new)\n",
    "        score = max(qa_f1(pred, ans) for ans in item['answers'])\n",
    "        scores.append(score * 100)\n",
    "        \n",
    "        if i == 0:\n",
    "            print(f'\\n[Sample 0] Pred: {pred[:200]}')\n",
    "            print(f'  Gold: {item[\"answers\"][:2]}')\n",
    "            print(f'  F1: {score:.2f}')\n",
    "    \n",
    "    return sum(scores)/len(scores)\n",
    "\n",
    "# ============= BASELINE TEST (NO PATCHING) =============\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE TEST: No patching (original model)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "@torch.no_grad()\n",
    "def baseline_test():\n",
    "    \"\"\"Test the base model WITHOUT patching to verify it works.\"\"\"\n",
    "    ds = load_dataset('THUDM/LongBench', 'qasper', split='test', trust_remote_code=True)\n",
    "    item = ds[0]\n",
    "    prompt = f\"You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \\\"unanswerable\\\". If the question is a yes/no question, answer \\\"yes\\\", \\\"no\\\", or \\\"unanswerable\\\". Do not provide any explanation.\\n\\nArticle: {item['context']}\\n\\n Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \\\"unanswerable\\\". If the question is a yes/no question, answer \\\"yes\\\", \\\"no\\\", or \\\"unanswerable\\\". Do not provide any explanation.\\n\\nQuestion: {item['input']}\\n\\nAnswer:\"\n",
    "    \n",
    "    inp = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=7500).to('cuda')\n",
    "    seq_len = inp.input_ids.shape[1]\n",
    "    print(f\"Input length: {seq_len} tokens\")\n",
    "    \n",
    "    out = base_model.generate(**inp, max_new_tokens=128, do_sample=False, pad_token_id=tokenizer.eos_token_id, use_cache=True)\n",
    "    result = tokenizer.decode(out[0][seq_len:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Prediction: {result[:300]}\")\n",
    "    print(f\"Gold: {item['answers'][:2]}\")\n",
    "    score = max(qa_f1(result, ans) for ans in item['answers'])\n",
    "    print(f\"F1 Score: {score:.2%}\")\n",
    "    return score\n",
    "\n",
    "baseline_score = baseline_test()\n",
    "print(f\"\\nBaseline F1: {baseline_score:.2%}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = {}\n",
    "for name, fn in ATOMS.items():\n",
    "    exp1[name] = eval_task('qasper', fn, BONDS['none'], COMBINES['static80'])\n",
    "    print(f'{name}: {exp1[name]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_atom = max(exp1, key=exp1.get)\n",
    "exp2 = {}\n",
    "for name, fn in BONDS.items():\n",
    "    exp2[name] = eval_task('qasper', ATOMS[best_atom], fn, COMBINES['max'])\n",
    "    print(f'{name}: {exp2[name]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bond = max(exp2, key=exp2.get)\n",
    "exp3 = {}\n",
    "for name, fn in COMBINES.items():\n",
    "    exp3[name] = eval_task('qasper', ATOMS[best_atom], BONDS[best_bond], fn)\n",
    "    print(f'{name}: {exp3[name]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combine = max(exp3, key=exp3.get)\n",
    "exp4 = {}\n",
    "for task in ['qasper', 'narrativeqa', 'hotpotqa']:\n",
    "    exp4[task] = eval_task(task, ATOMS[best_atom], BONDS[best_bond], COMBINES[best_combine], 30)\n",
    "    print(f'{task}: {exp4[task]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n=== RESULTS ===')\n",
    "print('Exp1 atoms:', exp1)\n",
    "print('Exp2 bonds:', exp2)\n",
    "print('Exp3 combine:', exp3)\n",
    "print('Exp4 tasks:', exp4)\n",
    "print(f'\\nBest: atom={best_atom}, bond={best_bond}, combine={best_combine}')\n",
    "print(f'Avg: {sum(exp4.values())/len(exp4):.2f}')\n",
    "json.dump({'exp1':exp1,'exp2':exp2,'exp3':exp3,'exp4':exp4,'best':{'atom':best_atom,'bond':best_bond,'combine':best_combine}}, \n",
    "          open('poc_results.json','w'), indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
