================================================================================
CIRCUITKV DEVELOPMENT CONTEXT - ICML 2026 SUBMISSION
================================================================================
Last Updated: January 2026
Target: Beat SnapKV (42.68) on LongBench benchmark
Model: Qwen/Qwen2.5-7B-Instruct, Budget: 2048 tokens

================================================================================
1. ALGORITHM OVERVIEW
================================================================================

CircuitKV is a KV cache compression method using Absorbing Markov Chain theory
to select which tokens to retain during LLM inference.

CORE MATHEMATICAL FRAMEWORK:
----------------------------
1. Attention matrix A defines transition probabilities P[i,j]
2. First `sink_size` tokens (default: 4) are "absorbing states" (always kept)
3. Remaining tokens are "transient states"
4. Q = P[sink:, sink:] is the transient-to-transient transition matrix
5. Fundamental matrix N = (I - Q)^(-1) computed via Neumann series:
   N ≈ I + Q + Q² + ... + Q^k  (k = neumann_iterations, default: 10)

TWO IMPORTANCE SIGNALS:
-----------------------
- QI (Query Importance): N[query_idx, j] = expected visits to token j FROM query
  - "High frequency" signal - captures precise, query-specific importance
  - Good for: Code completion, classification, exact retrieval

- HI (Hub Importance): mean(N[:, j]) = average visits to j from ALL positions
  - "Low frequency" signal - captures global hub/topic structure
  - Good for: Summarization, narrative understanding, context

COMBINATION FORMULA (v6.0.0+):
------------------------------
combined_scores = MAX(rank_normalize(QI), rank_normalize(HI))

This ensures tokens important in EITHER signal survive selection.

TOKEN SELECTION:
----------------
- Always keep: sink tokens (first sink_size) + local window (last window_size)
- Competitive slots: budget - sink_size - window_size
- Select top-k from middle region by combined_scores

================================================================================
2. VERSION HISTORY & EXPERIMENTS
================================================================================

| Version | Formula                              | Score | Gap   | Status    |
|---------|--------------------------------------|-------|-------|-----------|
| v4.5.0  | MAX(rank(QI×DA), rank(HI×DA))       | 42.24 | -0.44 | Baseline  |
| v5.0.0  | Union(QI, HI) - no DA                | 42.18 | -0.50 | Failed    |
| v5.1.0  | Union(QI×DA, HI×DA)                  | 42.15 | -0.53 | Failed    |
| v6.0.0  | MAX(rank(QI), rank(HI)) - no DA      | 42.42 | -0.26 | Success   |
| v6.1.0  | + Boxcar smoothing (k=5)             | 42.45 | -0.23 | Marginal  |
| v6.2.0  | + Asymmetric Gaussian (QI=3, HI=5)   | 42.53 | -0.15 | BEST      |
| v6.3.0  | + sink=64, iterations=15             | 42.48 | -0.20 | FAILED    |
| v6.4.0  | + Transition Sharpening (temp=0.3)   | TBD   | TBD   | PENDING   |

KEY FINDINGS:
-------------
1. DA (Degree-Aware) weighting HURTS performance (-0.18 points average)
   - DA = attention column sum (how much attention token receives)
   - Penalizes important hub tokens, helps 7 tasks but hurts 9 tasks

2. Uniform smoothing has mixed effects:
   - HELPS: qasper (+0.99), qmsum (+0.96), musique (+0.71)
   - HURTS: trec (-1.00), hotpotqa (-0.98), samsum (-0.84)
   - Problem: Blurs precision for code/classification tasks

3. "Frequency Separation" hypothesis VALIDATED in v6.2.0:
   - Sharp QI (kernel=3, tight sigma) preserves precision
   - Smooth HI (kernel=5, wide sigma) captures context
   - Gaussian > Boxcar (preserves center spike)

4. "Deep Field" hypothesis FAILED in v6.3.0:
   - Larger sink_size=64 wastes budget on early tokens
   - More iterations=15 causes signal diffusion (convergence to uniform)
   - Musique did NOT improve, passage_count dropped

================================================================================
3. CURRENT BEST: v6.2.0 CONFIGURATION
================================================================================

python run_longbench.py \
    --model_path Qwen/Qwen2.5-7B-Instruct \
    --method circuitkv \
    --max_capacity_prompts 2048 \
    --combination_mode dis \
    --use_gaussian \
    --qi_kernel_size 3 \
    --hi_kernel_size 5 \
    --gaussian_sigma 1.0 \
    --sink_size 4 \
    --neumann_iterations 10 \
    --window_size_override 64 \
    --attn_implementation flash_attention_2

PARAMETER MEANINGS:
-------------------
--combination_mode dis     : Dual-Importance Scoring (QI + HI, no DA)
--use_gaussian            : Use Gaussian kernel instead of boxcar
--qi_kernel_size 3        : Tight smoothing for QI (preserves precision)
--hi_kernel_size 5        : Wider smoothing for HI (captures context)
--gaussian_sigma 1.0      : Base sigma (QI uses sigma*0.5 internally)
--sink_size 4             : Absorbing boundary (first 4 tokens always kept)
--neumann_iterations 10   : Neumann series iterations for N approximation
--window_size_override 64 : Local attention window (last 64 tokens kept)

================================================================================
4. TASK-LEVEL ANALYSIS (v6.2.0 vs SnapKV)
================================================================================

WINS (CircuitKV better):
- repobench-p: +0.32 (26.60 vs 26.28) - Code completion
- passage_retrieval: +0.50 (99.50 vs 99.00) - Retrieval
- qmsum: +0.43 (23.27 vs 22.84) - Meeting summarization
- lcc: +0.17 (28.50 vs 28.33) - Code completion

TIES:
- trec: 67.00 = 67.00 - Classification
- passage_count: 12.00 = 12.00 - Counting

LOSSES (SnapKV better):
- musique: -1.11 (26.32 vs 27.43) - BIGGEST GAP, multi-hop reasoning
- 2wikimqa: -0.58 - Multi-hop QA
- narrativeqa: -0.56 - Narrative understanding
- hotpotqa: -0.35 - Multi-hop QA
- samsum: -0.50 - Dialogue summarization

PATTERN: Multi-hop reasoning tasks (musique, 2wikimqa, hotpotqa) are the
main remaining gap. These require following transitive paths across
distant tokens - the "bridge entity" problem.

================================================================================
5. FAILED HYPOTHESES
================================================================================

HYPOTHESIS: "Absorbing Boundary" (v6.3.0)
-----------------------------------------
Claim: sink_size=4 is too small, loses information at boundary
Fix: Increase sink_size to 64, iterations to 15
Result: FAILED - score dropped from 42.53 to 42.48

Why it failed:
1. Budget crowding: sink_size=64 consumes 60 extra tokens
   - Competitive slots: 1980 → 1920 (lost 3% of selection capacity)
   - passage_count dropped because a paragraph got cut

2. Signal diffusion: iterations=15 causes over-smoothing
   - Markov distribution converges toward stationary (uniform) distribution
   - QI loses specificity, looks like "background noise"
   - Musique DROPPED from 26.99 to 26.32

================================================================================
6. PENDING EXPERIMENT: v6.4.0 "Transition Sharpening"
================================================================================

HYPOTHESIS: Temperature scaling prevents signal diffusion
------------------------------------------------------------
Problem: As iterations increase, probability spreads out (diffusion)
Solution: Sharpen transition matrix BEFORE Markov walk

IMPLEMENTATION:
```python
# Standard: P = softmax(scores)
# Sharpened: P = softmax(scores / temperature)

# In code (pyramidkv_utils.py):
if temperature != 1.0 and temperature > 0:
    attn_scaled = attention ** (1.0 / temperature)  # Sharpening
else:
    attn_scaled = attention
P = attn_scaled / attn_scaled.sum(dim=1, keepdim=True)
```

With temperature=0.3:
- scores^(1/0.3) = scores^3.33
- High attention values get HIGHER (dominate)
- Low attention values get LOWER (suppressed)
- Random walker sticks to strongest reasoning paths

CONFIGURATION:
```bash
python run_longbench.py \
    --method circuitkv \
    --use_gaussian \
    --qi_kernel_size 3 \
    --hi_kernel_size 5 \
    --gaussian_sigma 1.0 \
    --sink_size 4 \
    --neumann_iterations 10 \
    --neumann_temperature 0.3 \
    --window_size_override 64
```

SLURM script: run_ckv_v64_transition_sharpening.sh

================================================================================
7. KEY CODE LOCATIONS
================================================================================

MAIN IMPLEMENTATION:
- KVCache-Factory/pyramidkv/pyramidkv_utils.py
  - Line ~1250: CircuitKVClusterManager class definition
  - Line ~1790: _compute_markov_importance_scores() - single signal
  - Line ~1910: _compute_dual_importance_scores() - QI + HI parallel
  - Line ~2140: _apply_smoothing() - Gaussian/boxcar convolution
  - Line ~2480: Version detection and logging
  - Line ~2550: _select_kv_markov_eviction_with_local() - main selection

CLI ARGUMENTS:
- KVCache-Factory/run_longbench.py
  - Line ~440: Smoothing arguments (--use_gaussian, --qi_kernel_size, etc.)
  - Line ~450: Markov tuning (--neumann_iterations, --sink_size, --neumann_temperature)
  - Line ~320: Config passing to model layers

SLURM SCRIPTS:
- run_ckv_v62_asymmetric_gaussian_v2.sh - Best configuration (v6.2.0)
- run_ckv_v63_deep_field.sh - Failed experiment
- run_ckv_v64_transition_sharpening.sh - Pending experiment

================================================================================
8. GAUSSIAN SMOOTHING IMPLEMENTATION
================================================================================

```python
def _apply_smoothing(self, scores, kernel_size, use_gaussian=False, sigma=1.0):
    if kernel_size <= 1:
        return scores

    scores_3d = scores.view(1, 1, -1)

    if use_gaussian:
        # Gaussian kernel - preserves center spike
        x = torch.arange(kernel_size) - (kernel_size - 1) / 2
        weight = torch.exp(-x**2 / (2 * sigma**2))
        weight = weight / weight.sum()
    else:
        # Boxcar kernel - uniform averaging
        weight = torch.ones(kernel_size) / kernel_size

    weight = weight.view(1, 1, -1)
    padding = kernel_size // 2
    smoothed = F.conv1d(scores_3d, weight, padding=padding)
    return smoothed.flatten()[:n]
```

Kernel comparison (k=5):
- Boxcar: [0.2, 0.2, 0.2, 0.2, 0.2] - 80% penalty to center
- Gaussian (σ=1.0): [0.1, 0.2, 0.4, 0.2, 0.1] - center dominates

================================================================================
9. REMAINING CHALLENGES
================================================================================

1. MUSIQUE GAP (-1.11 points)
   - Multi-hop: "Who is the spouse of the director of Film X?"
   - Need: Film X → Person Y (director) → Person Z (spouse)
   - Problem: "Person Y" may not score high in QI or HI individually

2. NARRATIVEQA GAP (-0.56 points)
   - Long narrative understanding
   - May need larger context window or different HI treatment

3. HOTPOTQA inconsistency
   - Sometimes helped by smoothing, sometimes hurt
   - May need task-adaptive configuration

POTENTIAL FUTURE DIRECTIONS:
----------------------------
a) Task-adaptive smoothing (detect task type, adjust parameters)
b) Weighted combination: α*QI + (1-α)*HI instead of MAX
c) Two-stage selection: HI candidates → QI re-ranking
d) Larger window_size (128 instead of 64)
e) Progressive filtering across layers

================================================================================
10. BASELINES FOR COMPARISON
================================================================================

SnapKV (target to beat): 42.68
H2O: 41.34
StreamingLLM: 35.87
PyramidKV: 41.97
Full attention (no compression): ~44-45

CircuitKV v6.2.0: 42.53 (gap: -0.15 to SnapKV)

================================================================================
11. ENVIRONMENT & INFRASTRUCTURE
================================================================================

- Cluster: SLURM with H200 GPUs
- Container: pytorch_24.11-py3.sif (Singularity)
- Model: Qwen/Qwen2.5-7B-Instruct
- Benchmark: LongBench (16 tasks)
- Attention: flash_attention_2

Key paths:
- /lustre/nvwulf/scratch/jungshwang/submission_folder/
- Results: KVCache-Factory/results/
- Logs: logs/

================================================================================
12. QUICK REFERENCE: WHAT TO TRY NEXT
================================================================================

1. Run v6.4.0 (temperature=0.3) - may recover Musique without diffusion
2. If v6.4.0 fails, try window_size=128 with v6.2.0 settings
3. Consider weighted combination: 0.7*QI + 0.3*HI instead of MAX
4. Analyze Musique examples to understand where bridge entities are lost

COMMAND TO RUN BEST CONFIG:
```bash
sbatch run_ckv_v62_asymmetric_gaussian_v2.sh  # Current best: 42.53
sbatch run_ckv_v64_transition_sharpening.sh   # Pending: temperature=0.3
```

================================================================================
END OF CONTEXT
================================================================================
